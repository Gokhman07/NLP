{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MODEL CREATION"
      ],
      "metadata": {
        "id": "nbE0V1KIxKUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to create Siamese model\n",
        "def create_siamese_model(vocab_size, embedding_dim, lstm_units):\n",
        "    # Shared embedding layer\n",
        "    input_layer = Input(shape=(None,))\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim)\n",
        "    lstm_layer = LSTM(lstm_units)\n",
        "    embedded_sequence = lstm_layer(embedding_layer(input_layer))\n",
        "\n",
        "    shared_lstm_model = Model(inputs=input_layer, outputs=embedded_sequence)\n",
        "\n",
        "    # Two input sentences\n",
        "    input_sentence1 = Input(shape=(None,))\n",
        "    input_sentence2 = Input(shape=(None,))\n",
        "\n",
        "    # Process sentences using the shared LSTM model\n",
        "    embedded_sentence1 = shared_lstm_model(input_sentence1)\n",
        "    embedded_sentence2 = shared_lstm_model(input_sentence2)\n",
        "\n",
        "    # Compute cosine similarity between the sentence embeddings\n",
        "    def cosine_distance(inputs):\n",
        "        x, y = inputs\n",
        "        x = tf.math.l2_normalize(x, axis=-1)\n",
        "        y = tf.math.l2_normalize(y, axis=-1)\n",
        "        return tf.reduce_sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "    distance = Lambda(cosine_distance)([embedded_sentence1, embedded_sentence2])\n",
        "\n",
        "    # Final output layer for similarity score\n",
        "    output_layer = Dense(1, activation='sigmoid')(distance)\n",
        "\n",
        "    siamese_model = Model(inputs=[input_sentence1, input_sentence2], outputs=output_layer)\n",
        "\n",
        "    return siamese_model\n",
        "\n"
      ],
      "metadata": {
        "id": "D_6uhBp3xMbl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "cgurQg9Axmxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Example data for training\n",
        "sentence_pairs = [\n",
        "    (\"I love coding with Python.\", \"Python programming is my favorite.\"),\n",
        "    (\"Machine learning is fascinating.\", \"I enjoy studying ML algorithms.\"),\n",
        "    (\"The sky is blue.\", \"Grass is green.\"),\n",
        "    (\"Reading books is relaxing.\", \"I like to read novels.\"),\n",
        "]\n",
        "\n",
        "# Generate vocabulary and prepare data\n",
        "vocab = set()\n",
        "for sent1, sent2 in sentence_pairs:\n",
        "  #  vocab.update(sent1.split())\n",
        "    vocab.update(sent2.split())\n",
        "\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "max_sequence_length = max(len(sent.split()) for sent1, sent2 in sentence_pairs for sent in [sent1, sent2])\n",
        "\n",
        "# Convert sentences to sequences of word indices\n",
        "def convert_to_sequences(sentence_pairs, word_to_index, max_sequence_length):\n",
        "    sentence1_sequences = []\n",
        "    sentence2_sequences = []\n",
        "    for sent1, sent2 in sentence_pairs:\n",
        "        sent1_seq = [word_to_index[word] for word in sent1.split()]\n",
        "        sent2_seq = [word_to_index[word] for word in sent2.split()]\n",
        "        pad_length = max_sequence_length - len(sent1_seq)\n",
        "        sentence1_sequences.append(sent1_seq + [0] * pad_length)\n",
        "        pad_length = max_sequence_length - len(sent2_seq)\n",
        "        sentence2_sequences.append(sent2_seq + [0] * pad_length)\n",
        "    return np.array(sentence1_sequences), np.array(sentence2_sequences)\n",
        "\n",
        "sentence1_sequences, sentence2_sequences = convert_to_sequences(sentence_pairs, word_to_index, max_sequence_length)"
      ],
      "metadata": {
        "id": "tEC129v5xouj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN DATA"
      ],
      "metadata": {
        "id": "7v0ptu8nxq7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model hyperparameters\n",
        "vocab_size = len(vocab) + 1\n",
        "embedding_dim = 50\n",
        "lstm_units = 64\n",
        "\n",
        "# Create and compile Siamese model\n",
        "siamese_model = create_siamese_model(vocab_size, embedding_dim, lstm_units)\n",
        "siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the Siamese model\n",
        "siamese_model.fit([sentence1_sequences, sentence2_sequences], np.array([1, 1, 0, 0]), epochs=10, batch_size=2)\n",
        "\n",
        "# Now the Siamese model is trained and ready to be used for similarity measurement.\n"
      ],
      "metadata": {
        "id": "C-NLVRTAxtzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4AsFPw-wEQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}